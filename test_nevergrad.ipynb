{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Other Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array{(2,)}:[0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "import nevergrad as ng\n",
    "\n",
    "def square(x):\n",
    "    return sum((x - 0.5) ** 2)\n",
    "\n",
    "# optimization on x as an array of shape (2,)\n",
    "optimizer = ng.optimizers.NGOpt(parametrization=2, budget=100)\n",
    "res = optimizer.minimize(square)  # best value\n",
    "print(res)\n",
    "# >>> [0.49999998 0.50000004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ParametrizedTBPSA' object has no attribute 'minimize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[46], line 10\u001b[0m\n",
      "\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# optimization on x as an array of shape (2,)\u001b[39;00m\n",
      "\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m ng\u001b[38;5;241m.\u001b[39mfamilies\u001b[38;5;241m.\u001b[39mParametrizedTBPSA(naive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, initial_popsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;32m---> 10\u001b[0m recommendation \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m(square)  \u001b[38;5;66;03m# best value\u001b[39;00m\n",
      "\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(recommendation\u001b[38;5;241m.\u001b[39mvalue)\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# >>> [0.49971112 0.5002944 ]\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ParametrizedTBPSA' object has no attribute 'minimize'"
     ]
    }
   ],
   "source": [
    "import nevergrad as ng\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def square(x):\n",
    "    return sum((x) ** 2)\n",
    "\n",
    "# optimization on x as an array of shape (2,)\n",
    "optimizer = ng.families.ParametrizedTBPSA(naive=False, initial_popsize=4)\n",
    "recommendation = optimizer.minimize(square)  # best value\n",
    "print(recommendation.value)\n",
    "# >>> [0.49971112 0.5002944 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nevergrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nevergrad as ng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 started\n",
      "Task 1 started\n",
      "Task 2 started\n",
      "Task 3 started\n",
      "Task 1 done\n",
      "Task 4 started\n",
      "Task 2 done\n",
      "Task 5 started\n",
      "Task 0 done\n",
      "Task 6 started\n",
      "Task 3 done\n",
      "Task 4 done\n",
      "Task 5 done\n",
      "Task 6 done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.cpu_count\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def task(n):\n",
    "    print(f\"Task {n} started\")\n",
    "    time.sleep(2)\n",
    "    print(f\"Task {n} done\")\n",
    "    return n * n\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    futures = [executor.submit(task, i) for i in range(7)]\n",
    "    \n",
    "# print(futures)\n",
    "# for future in futures:\n",
    "#     print(f\"Result: {future}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/miniforge3/envs/optimagic/lib/python3.10/site-packages/nevergrad/optimization/base.py:149: InefficientSettingsWarning: num_workers = 2 > 1 is suboptimal when run sequentially\n",
      "  warnings.warn(msg, e)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([0.67669304, 0.43679202]),), {})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nevergrad as ng\n",
    "\n",
    "def square(x):\n",
    "    return sum((x - 0.5) ** 2)\n",
    "\n",
    "instrum = ng.p.Instrumentation(ng.p.Array(shape=(2,)) ) # We are working on R^2 x R.\n",
    "optimizer = ng.optimizers.ConfPSO()(parametrization=instrum, budget=50, num_workers=2)\n",
    "\n",
    "optimizer.minimize(square).value\n",
    "# x = optimizer.ask()\n",
    "# x_list = [optimizer.ask() for _ in range(optimizer.num_workers)]\n",
    "# print(x_list[0].value[0][0])\n",
    "# from optimagic.optimization.internal_optimization_problem import problem\n",
    "# losses = problem.batch_fun(\n",
    "#         [x.value[0][0] for x in x_list], n_cores=5\n",
    "#     )\n",
    "# for _ in range(optimizer.budget):\n",
    "#     x = optimizer.ask()\n",
    "#     loss = square(*x.args, **x.kwargs)\n",
    "#     print(x,loss)\n",
    "#     optimizer.tell(x, loss)\n",
    "\n",
    "# recommendation = optimizer.provide_recommendation()\n",
    "# print(recommendation.value)\n",
    "# print(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Benchmark] Running with n_cores=1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ParametrizedCMA.__init__() got an unexpected keyword argument 'cma_options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[14], line 72\u001b[0m\n",
      "\u001b[1;32m     69\u001b[0m stopping_maxfun \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Run benchmark\u001b[39;00m\n",
      "\u001b[0;32m---> 72\u001b[0m benchmark_results \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark_pso_by_cores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_cores_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopping_maxfun\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Print final summary\u001b[39;00m\n",
      "\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m benchmark_results:\n",
      "\n",
      "Cell \u001b[0;32mIn[14], line 48\u001b[0m, in \u001b[0;36mbenchmark_pso_by_cores\u001b[0;34m(n_cores_list, stopping_maxfun)\u001b[0m\n",
      "\u001b[1;32m     41\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m NevergradCMAES(\n",
      "\u001b[1;32m     42\u001b[0m     n_cores\u001b[38;5;241m=\u001b[39mn_cores,\n",
      "\u001b[1;32m     43\u001b[0m     stopping_maxfun\u001b[38;5;241m=\u001b[39mstopping_maxfun\n",
      "\u001b[1;32m     44\u001b[0m )\n",
      "\u001b[1;32m     46\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;32m---> 48\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreally_hard_fun\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\n",
      "\u001b[1;32m     52\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;32m     54\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;32m     55\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "\n",
      "File \u001b[0;32m~/Documents/optimagic/src/optimagic/optimization/optimize.py:479\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, params, algorithm, bounds, constraints, fun_kwargs, algo_options, jac, jac_kwargs, fun_and_jac, fun_and_jac_kwargs, numdiff_options, logging, error_handling, error_penalty, scaling, multistart, collect_history, skip_checks, x0, method, args, hess, hessp, callback, options, tol, criterion, criterion_kwargs, derivative, derivative_kwargs, criterion_and_derivative, criterion_and_derivative_kwargs, log_options, lower_bounds, upper_bounds, soft_lower_bounds, soft_upper_bounds, scaling_options, multistart_options)\u001b[0m\n",
      "\u001b[1;32m    332\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Minimize criterion using algorithm subject to constraints.\u001b[39;00m\n",
      "\u001b[1;32m    333\u001b[0m \n",
      "\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    429\u001b[0m \n",
      "\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    432\u001b[0m problem \u001b[38;5;241m=\u001b[39m create_optimization_problem(\n",
      "\u001b[1;32m    433\u001b[0m     direction\u001b[38;5;241m=\u001b[39mDirection\u001b[38;5;241m.\u001b[39mMINIMIZE,\n",
      "\u001b[1;32m    434\u001b[0m     fun\u001b[38;5;241m=\u001b[39mfun,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    477\u001b[0m     multistart_options\u001b[38;5;241m=\u001b[39mmultistart_options,\n",
      "\u001b[1;32m    478\u001b[0m )\n",
      "\u001b[0;32m--> 479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/optimagic/src/optimagic/optimization/optimize.py:633\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(problem)\u001b[0m\n",
      "\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# TODO: Actually use the step ids\u001b[39;00m\n",
      "\u001b[1;32m    628\u001b[0m     step_id \u001b[38;5;241m=\u001b[39m log_scheduled_steps_and_get_ids(  \u001b[38;5;66;03m# noqa: F841\u001b[39;00m\n",
      "\u001b[1;32m    629\u001b[0m         steps\u001b[38;5;241m=\u001b[39msteps,\n",
      "\u001b[1;32m    630\u001b[0m         logger\u001b[38;5;241m=\u001b[39mlogger,\n",
      "\u001b[1;32m    631\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;32m--> 633\u001b[0m     raw_res \u001b[38;5;241m=\u001b[39m \u001b[43mproblem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve_internal_problem\u001b[49m\u001b[43m(\u001b[49m\u001b[43minternal_problem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    636\u001b[0m     multistart_options \u001b[38;5;241m=\u001b[39m get_internal_multistart_options_from_public(\n",
      "\u001b[1;32m    637\u001b[0m         options\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mmultistart,\n",
      "\u001b[1;32m    638\u001b[0m         params\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mparams,\n",
      "\u001b[1;32m    639\u001b[0m         params_to_internal\u001b[38;5;241m=\u001b[39mconverter\u001b[38;5;241m.\u001b[39mparams_to_internal,\n",
      "\u001b[1;32m    640\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m~/Documents/optimagic/src/optimagic/optimization/algorithm.py:282\u001b[0m, in \u001b[0;36mAlgorithm.solve_internal_problem\u001b[0;34m(self, problem, x0, step_id)\u001b[0m\n",
      "\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m problem\u001b[38;5;241m.\u001b[39mlogger:\n",
      "\u001b[1;32m    278\u001b[0m     problem\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mstep_store\u001b[38;5;241m.\u001b[39mupdate(\n",
      "\u001b[1;32m    279\u001b[0m         step_id, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(StepStatus\u001b[38;5;241m.\u001b[39mRUNNING\u001b[38;5;241m.\u001b[39mvalue)}\n",
      "\u001b[1;32m    280\u001b[0m     )\n",
      "\u001b[0;32m--> 282\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_solve_internal_problem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_info\u001b[38;5;241m.\u001b[39mdisable_history) \u001b[38;5;129;01mand\u001b[39;00m (result\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;32m    285\u001b[0m     result \u001b[38;5;241m=\u001b[39m replace(result, history\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mhistory)\n",
      "\n",
      "File \u001b[0;32m~/Documents/optimagic/src/optimagic/optimizers/nevergrad_optimizers.py:69\u001b[0m, in \u001b[0;36mNevergradCMAES._solve_internal_problem\u001b[0;34m(self, problem, x0)\u001b[0m\n",
      "\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_solve_internal_problem\u001b[39m(\n",
      "\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m, problem: InternalOptimizationProblem, x0: NDArray[np\u001b[38;5;241m.\u001b[39mfloat64]\n",
      "\u001b[1;32m     61\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InternalOptimizeResult:\n",
      "\u001b[1;32m     62\u001b[0m     cma_options \u001b[38;5;241m=\u001b[39m {\n",
      "\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxtol,\n",
      "\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolfun\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mftol,\n",
      "\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCMA_rankmu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate_rank_mu_update,\n",
      "\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCMA_rankone\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate_rank_one_update,\n",
      "\u001b[1;32m     67\u001b[0m     }\n",
      "\u001b[0;32m---> 69\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParametrizedCMA\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpopsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopulation_size\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43melitist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melitist\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiagonal\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhigh_speed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhigh_speed\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfcmaes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_fast_implementation\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_init\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcma_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcma_options\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     80\u001b[0m     res \u001b[38;5;241m=\u001b[39m nevergrad_internal(\n",
      "\u001b[1;32m     81\u001b[0m         problem\u001b[38;5;241m=\u001b[39mproblem,\n",
      "\u001b[1;32m     82\u001b[0m         x0\u001b[38;5;241m=\u001b[39mx0,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     85\u001b[0m         n_cores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_cores,\n",
      "\u001b[1;32m     86\u001b[0m     )\n",
      "\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: ParametrizedCMA.__init__() got an unexpected keyword argument 'cma_options'"
     ]
    }
   ],
   "source": [
    "import optimagic as om\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import time\n",
    "import numpy as np\n",
    "import timeparametrizationparametrization\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def really_hard_fun(x):\n",
    "    \"\"\"Heavy numerical linear algebra work.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    x = np.asarray(x).flatten()  # Ensure x is a 1D numpy array\n",
    "    size = len(x)\n",
    "\n",
    "    if size < 2:\n",
    "        raise ValueError(\"Input vector 'x' must have at least 2 elements\")\n",
    "\n",
    "    # Create a positive-definite matrix\n",
    "    matrix = np.outer(x, x) + np.eye(size)\n",
    "\n",
    "    # Eigenvalue decomposition\n",
    "    eigvals = np.linalg.eigvals(matrix)\n",
    "\n",
    "    # Sum of squares of eigenvalues\n",
    "    result = np.sum(eigvals**2)\n",
    "\n",
    "    # Simulate time delay\n",
    "    time.sleep(0.2)  # 200 ms delay\n",
    "\n",
    "    return result\n",
    "\n",
    "def benchmark_pso_by_cores(n_cores_list, stopping_maxfun):\n",
    "    results = []\n",
    "\n",
    "    for n_cores in n_cores_list:\n",
    "        print(f\"\\n[Benchmark] Running with n_cores={n_cores}\")\n",
    "\n",
    "        optimizer = NevergradCMAES(\n",
    "            n_cores=n_cores,\n",
    "            stopping_maxfun=stopping_maxfun\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        result = om.minimize(\n",
    "            fun=really_hard_fun,\n",
    "            params=np.array([0]*300),\n",
    "            algorithm=optimizer\n",
    "        ).x.round(2)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"[Benchmark] Result: {result}, Time: {elapsed_time:.2f}s\")\n",
    "\n",
    "        results.append({\n",
    "            'n_cores': n_cores,\n",
    "            'result': result,\n",
    "            'time': elapsed_time\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test values\n",
    "n_cores_list = [1, 2,4,5,10,20,50]\n",
    "stopping_maxfun = 200\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = benchmark_pso_by_cores(n_cores_list, stopping_maxfun)\n",
    "\n",
    "# Print final summary\n",
    "for r in benchmark_results:\n",
    "    print(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AdaptSigma': 'True  # or False or any CMAAdaptSigmaBase class e.g. CMAAdaptSigmaTPA, CMAAdaptSigmaCSA',\n",
       " 'CMA_active': 'True  # negative update, conducted after the original update',\n",
       " 'CMA_active_injected': '0  #v weight multiplier for negative weights of injected solutions',\n",
       " 'CMA_cmean': '1  # learning rate for the mean value',\n",
       " 'CMA_const_trace': 'False  # normalize trace, 1, True, \"arithm\", \"geom\", \"aeig\", \"geig\" are valid',\n",
       " 'CMA_diagonal': '0*100*N/popsize**0.5  # nb of iterations with diagonal covariance matrix, True for always',\n",
       " 'CMA_diagonal_decoding': '0  # learning rate multiplier for additional diagonal update',\n",
       " 'CMA_eigenmethod': 'np.linalg.eigh  # or cma.utilities.math.eig or pygsl.eigen.eigenvectors',\n",
       " 'CMA_elitist': 'False  #v or \"initial\" or True, elitism likely impairs global search performance',\n",
       " 'CMA_injections_threshold_keep_len': '1  #v keep length if Mahalanobis length is below the given relative threshold',\n",
       " 'CMA_mirrors': 'popsize < 6  # values <0.5 are interpreted as fraction, values >1 as numbers (rounded), for `True` about 0.16 is used',\n",
       " 'CMA_mirrormethod': '2  # 0=unconditional, 1=selective, 2=selective with delay',\n",
       " 'CMA_mu': 'None  # parents selection parameter, default is popsize // 2',\n",
       " 'CMA_on': '1  # multiplier for all covariance matrix updates',\n",
       " 'CMA_sampler': 'None  # a class or instance that implements the interface of `cma.interfaces.StatisticalModelSamplerWithZeroMeanBaseClass`',\n",
       " 'CMA_sampler_options': '{}  # options passed to `CMA_sampler` class init as keyword arguments',\n",
       " 'CMA_rankmu': '1.0  # multiplier for rank-mu update learning rate of covariance matrix',\n",
       " 'CMA_rankone': '1.0  # multiplier for rank-one update learning rate of covariance matrix',\n",
       " 'CMA_recombination_weights': 'None  # a list, see class RecombinationWeights, overwrites CMA_mu and popsize options',\n",
       " 'CMA_dampsvec_fac': 'np.inf  # tentative and subject to changes, 0.5 would be a \"default\" damping for sigma vector update',\n",
       " 'CMA_dampsvec_fade': '0.1  # tentative fading out parameter for sigma vector update',\n",
       " 'CMA_teststds': 'None  # factors for non-isotropic initial distr. of C, mainly for test purpose, see CMA_stds for production',\n",
       " 'CMA_stds': 'None  # multipliers for sigma0 in each coordinate (not represented in C), or use `cma.ScaleCoordinates` instead',\n",
       " 'CSA_dampfac': '1  #v positive multiplier for step-size damping, 0.3 is close to optimal on the sphere',\n",
       " 'CSA_damp_mueff_exponent': 'None  # exponent for mueff/N, by default 0.5 and 1 if CSA_squared, zero means no dependency of damping on mueff, useful with CSA_disregard_length option',\n",
       " 'CSA_disregard_length': 'False  #v True is untested, also changes respective parameters',\n",
       " 'CSA_clip_length_value': 'None  #v poorly tested, [0, 0] means const length N**0.5, [-1, 1] allows a variation of +- N/(N+2), etc.',\n",
       " 'CSA_squared': 'False  #v use squared length for sigma-adaptation ',\n",
       " 'CSA_invariant_path': 'False  #v pc is invariant and ps (default) is unbiased',\n",
       " 'stall_sigma_change_on_divergence_iterations': 'False  #v number of iterations of median worsenings threshold at which the sigma change is stalled; the default may become 2',\n",
       " 'BoundaryHandler': 'BoundTransform  # or BoundPenalty, unused when ``bounds in (None, [None, None])``',\n",
       " 'bounds': '[None, None]  # lower (=bounds[0]) and upper domain boundaries, each a scalar or a list/vector',\n",
       " 'conditioncov_alleviate': '[1e8, 1e12]  # when to alleviate the condition in the coordinates and in main axes',\n",
       " 'eval_final_mean': 'True  # evaluate the final mean, which is a favorite return candidate',\n",
       " 'fixed_variables': 'None  # dictionary with index-value pairs like {0:1.1, 2:0.1} that are not optimized',\n",
       " 'ftarget': '-inf  #v target function value, minimization',\n",
       " 'integer_variables': '[]  # index list, invokes basic integer handling by setting minstd of integer variables if it was not given and by integer centering',\n",
       " 'is_feasible': 'is_feasible  #v a function that computes feasibility, by default lambda x, f: f not in (None, np.nan)',\n",
       " 'maxfevals': 'inf  #v maximum number of function evaluations',\n",
       " 'maxiter': '100 + 150 * (N+3)**2 // popsize**0.5  #v maximum number of iterations',\n",
       " 'mean_shift_line_samples': 'False #v sample two new solutions colinear to previous mean shift',\n",
       " 'mindx': '0  #v minimal std in any arbitrary direction, cave interference with tol*',\n",
       " 'minstd': '0  #v minimal std (scalar or vector) in any coordinate direction, cave interference with tol*',\n",
       " 'maxstd': 'None  #v maximal std (scalar or vector) in any coordinate direction',\n",
       " 'maxstd_boundrange': '1/3  # maximal std relative to bound_range per coordinate, overruled by maxstd',\n",
       " 'pc_line_samples': 'False #v one line sample along the evolution path pc',\n",
       " 'popsize': '4 + 3 * np.log(N)  # population size, AKA lambda, int(popsize) is the number of new solution per iteration',\n",
       " 'popsize_factor': '1  # multiplier for popsize, convenience option to increase default popsize',\n",
       " 'randn': 'np.random.randn  #v randn(lam, N) must return an np.array of shape (lam, N), see also cma.utilities.math.randhss',\n",
       " 'scaling_of_variables': 'None  # deprecated, rather use fitness_transformations.ScaleCoordinates instead (or CMA_stds). WAS: Scale for each variable in that effective_sigma0 = sigma0*scaling. Internally the variables are divided by scaling_of_variables and sigma is unchanged, default is `np.ones(N)`',\n",
       " 'seed': 'time  # random number seed for `numpy.random`; `None` and `0` equate to `time`, `np.nan` means \"do nothing\", see also option \"randn\"',\n",
       " 'signals_filename': 'cma_signals.in  # read versatile options from this file (use `None` or `\"\"` for no file) which contains a single options dict, e.g. ``{\"timeout\": 0}`` to stop, string-values are evaluated, e.g. \"np.inf\" is valid',\n",
       " 'termination_callback': '[]  #v a function or list of functions returning True for termination, called in `stop` with `self` as argument, could be abused for side effects',\n",
       " 'timeout': 'inf  #v stop if timeout seconds are exceeded, the string \"2.5 * 60**2\" evaluates to 2 hours and 30 minutes',\n",
       " 'tolconditioncov': '1e14  #v stop if the condition of the covariance matrix is above `tolconditioncov`',\n",
       " 'tolfacupx': '1e3  #v termination when step-size increases by tolfacupx (diverges). That is, the initial step-size was chosen far too small and better solutions were found far away from the initial solution x0',\n",
       " 'tolupsigma': '1e20  #v sigma/sigma0 > tolupsigma * max(eivenvals(C)**0.5) indicates \"creeping behavior\" with usually minor improvements',\n",
       " 'tolflatfitness': '1  #v iterations tolerated with flat fitness before termination',\n",
       " 'tolfun': '1e-11  #v termination criterion: tolerance in function value, quite useful',\n",
       " 'tolfunhist': '1e-12  #v termination criterion: tolerance in function value history',\n",
       " 'tolfunrel': '0  #v termination criterion: relative tolerance in function value: Delta f current < tolfunrel * (median0 - median_min)',\n",
       " 'tolstagnation': 'int(100 + 100 * N**1.5 / popsize)  #v termination if no improvement over tolstagnation iterations',\n",
       " 'tolxstagnation': '[1e-9, 20, 0.1]  #v termination thresholds for Delta of [mean, iterations, iterations fraction], the latter two are summed; trigger termination if Dmean stays below the threshold over Diter iterations, pass `False` or a negative value to turn off tolxstagnation',\n",
       " 'tolx': '1e-11  #v termination criterion: tolerance in x-changes',\n",
       " 'transformation': 'None  # deprecated, use a wrapper like those in cma.fitness_transformations instead.',\n",
       " 'typical_x': 'None  # deprecated, use `cma.fitness_transformations.Shifted` instead',\n",
       " 'updatecovwait': 'None  #v number of iterations without distribution update, name is subject to future changes',\n",
       " 'verbose': '3  #v verbosity e.g. of initial/final message, -1 is very quiet, -9 maximally quiet, may not be fully implemented',\n",
       " 'verb_append': '0  # initial evaluation counter, if append, do not overwrite output files',\n",
       " 'verb_disp': '100  #v verbosity: display console output every verb_disp iteration',\n",
       " 'verb_disp_overwrite': 'inf  #v start overwriting after given iteration',\n",
       " 'verb_filenameprefix': 'outcmaes/  # output path (folder) and filenames prefix',\n",
       " 'verb_log': '1  #v verbosity: write data to files every verb_log iteration, writing can be time critical on fast to evaluate functions',\n",
       " 'verb_log_expensive': 'N * (N <= 50)  # allow to execute eigendecomposition for logging every verb_log_expensive iteration, 0 or False for never',\n",
       " 'verb_plot': '0  #v in fmin2(): plot() is called every verb_plot iteration',\n",
       " 'verb_time': 'True  #v output timings on console',\n",
       " 'vv': '{}  #? versatile set or dictionary for hacking purposes, value found in self.opts[\"vv\"]'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cma\n",
    "opts = cma.CMAOptions()\n",
    "opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def square(x):\n",
    "    return sum((x - 0.5) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46820812 0.37855092]\n"
     ]
    }
   ],
   "source": [
    "from optimagic.optimizers.nevergrad_optimizers import NevergradCMAES#, CustomExecutor\n",
    "import nevergrad as ng\n",
    "import numpy as np\n",
    "import time\n",
    "import optimagic as om\n",
    "start_time = time.time()\n",
    "res = om.minimize(fun=square,\n",
    "            params = np.array([0,0]),\n",
    "            algorithm=NevergradCMAES(n_cores=1,seed=12,population_size=24),\n",
    "           )\n",
    "# square(np.array([1,1]))\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "# print(elapsed_time)\n",
    "print(res.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "instrum = ng.p.Instrumentation(\n",
    "    ng.p.Array(shape=(2,))\n",
    ")\n",
    "instrum.random_state.seed(12)\n",
    "optimizer = ng.optimizers.ParametrizedCMA(diagonal=True)(parametrization=instrum, budget=100,)\n",
    "# optimizer = ng.optimizers.ParametrizedTBPSA()(parametrization=instrum, budget=10)\n",
    "recommendation = optimizer.minimize(square)\n",
    "print(recommendation.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'callbacks', 'common', 'errors', 'families', 'ops', 'optimization', 'optimizers', 'p', 'parametrization', 'typing']\n"
     ]
    }
   ],
   "source": [
    "import nevergrad as ng\n",
    "print(dir(ng))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
